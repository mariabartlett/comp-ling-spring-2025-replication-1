---
title: "Replication Code: A Replication of _Measuring and Explaining Political Sophistication through Textual Complexity_ (Benoit et al., 2019)"
subtitle: "PPOL 6801: Text as Data: Computational Linguistics"
author: "Wendy Shi & Maria Bartlett"
date: "`r Sys.Date()`"
format: html
theme: litera
toc: TRUE
toc-location: left
toc-depth: 7
embed-resources: TRUE
linkcolor: "black"
editor: visual
fontsize: 12pt
css: bootstrap.css
page-layout: full
---

### Set-up

```{r}
#| message: FALSE
#| warning: FALSE
#| code-fold: TRUE

# clear global environment
rm(list = ls())

# set seed
set.seed(12345)

# provide instruction for how to install user-written packages
#devtools::install_github("quanteda/spacyr", build_vignettes = FALSE)
#library(spacyr)
#spacy_install()
#spacy_initialize()
#devtools::install_github("kbenoit/sophistication")

# load packages
library(tidyverse)
library(spacyr)
library(sophistication)  
library(BradleyTerry2)
library(modelsummary)
library(quanteda)
library(quanteda.textstats)
library(assertr)
library(DT)
library(randomForest) 
library(kableExtra)

# set processing/display options
options(mc.cores = parallel::detectCores())
options(dplyr.summarise.inform = FALSE)
options(warn = -1)

# set relative paths
root       <- file.path(getwd() %>% dirname())
author_rep <- file.path(root,"02_DataverseMaterials","dataverse_files")

```

### Step 1. Load Crowdsourced data

As directly quoted from `Codebook.pdf` in Benoit, Munger, and Spirling's replication package:

"These are the coded snippet comparisons that were downloaded from Crowdflower after the crowd-sourced job was completed."

Key variables (definitions quoted directly from `Codebook.pdf`):

-   `docID1`: "The document ID for the text from which first snippet was taken, e.g. "Bush-2005""

-   `snippetID1`: "A unique numeric snippet identifier for snippet 1"

-   `text1:` "The plain text of snippet 1"

-   same `docID2`,`snippedID2`, `text2` for snippet 2

-   `X_golden`: "a logical value (true or false) indicating whether the snippet pair was a "gold" question for which we supplied an answer"

-   `screener`: "a logical value TRUE indicating whether the question was a "screener" (a special gold question with embedded instructions to the coder as to how to answer the task) or blank if the question was not a screener."

-   `X_id`: "a unique numeric identifier for the answer, assigned by Crowdflower (e.g. 2025513496)"

-   `X_missed`: "a Boolean (true or blank) indicating whether the crowd worker missed a gold question"

-   `X_tainted`: "a Boolean (true or false) indicating whether the answer was "tainted" because the worker missed too many screening questions. Because we excluded tainted answers, in our data, all values of this variable are false."

-   `X_trust`: "the "trust" score for the respondent, as computed by Crowdflower. Our minimum was 0.60 and our mean answer had a trust value of around 0.86."

-   `X_worker_id`: "a unique numeric identifier for the worker, e.g. 34616922"

-   `easier`: "1 or 2, indicating which snippet was answered as being easier by the worker. This is the core data we used in scoring the snippets."

```{r}
#| warning: false
#| output: asis
#| code-fold: false

root       <- file.path(getwd() %>% dirname())
author_rep <- file.path(root,"02_DataverseMaterials","dataverse_files")


# load human-coded data of SOTU passages from CrowdFlower
allsentences <- rbind(read.csv(file.path(author_rep,"CF_output_f999866.csv"), stringsAsFactors = FALSE),
                      read.csv(file.path(author_rep,"CF_output_f952737.csv"), stringsAsFactors = FALSE)) %>%
  # confirm file is unique on id level
  verify(anyDuplicated(select(.,c(X_id))) == 0) %>%
  # check dimensions of data
  verify(nrow(.) == 27807 & ncol(.) == 25) %>%
  # confirm all tainted responses have been removed, per codebook
  verify(X_tainted == "false")

# output a few examples of text comparisons
allsentences %>%
  select(X_id,X_worker_id,snippetid1,docid1,text1,snippetid2,docid2,text2,easier) %>%
  distinct() %>%
  slice_head(n = 5) %>%
  datatable()

```

### Step 2. Prepare data for Bradley-Terry model

Authors assume a complex text uses:

-   Lengthier words more often

-   Rarer words more often (where "rare" differs temporally)

-   Lengthier sentences

-   Advanced grammar and syntax organization (e.g., subordinate clauses)

```{r}
#| warning: false
#| output: asis
#| code-fold: false

# add covariates to prepare data for BT Model
job999866covars_chameleons <-
    bt_input_make(allsentences, 
                  # add covariates for each snippet, taken directly from Crowdflower saved data
                  covars = TRUE,
                  # specify measures of text difficulty
                  readability_measure = c("Flesch", 
                                          "Dale.Chall",
                                          "FOG", 
                                          "SMOG",
                                          "Spache",
                                          "Coleman.Liau"),
                  # measures of word rarity (Google and Brown)
                  covars_baseline = TRUE, 
                  # measures of parts of speech
                  covars_pos = TRUE, 
                  # normalize covariates
                  normalize = TRUE)

# view 'easier' data: at the pairwise-comparison level, contains an ID with the unique snippet ID of the 
# ID that won the comparison
job999866covars_chameleons$easier %>%
  verify(nrow(.) == 19430) %>%
  head() %>%
  datatable()

# view 'harder' data: at the pairwise-comparison level, contains an ID with the unique snippet ID of the 
# ID that lost the comparison
job999866covars_chameleons$harder %>%
  verify(nrow(.) == 19430) %>%  # same number of records as easier dataset because still at pairwise-comparison level
  head() %>%
  datatable()

# view 'predictors' data: data with predictors associated with each distinct snippet, where the id of the row.name
# corresponds with the ID in the 'easier' and 'harder' dataframes
job999866covars_chameleons$predictors %>%
  # confirm unique at snippet level (which is now called doc_id)
  verify(anyDuplicated(select(.,c(doc_id))) == 0) %>%
  head() %>%
  datatable(options = list(scrollX='400px'))

```

#### Step 2.1. Data understanding checks

```{r}
#| warning: false
#| output: asis
#| code-fold: false

# extract dataframe with covariates
pred_data <- job999866covars_chameleons$predictors %>%
  mutate(easier_id = row.names(.),
         harder_id = row.names(.))

# extract snippet 1 from each comparison
snippet_data1 <- allsentences %>%
  select(snippetid1,text1) %>%
  rename(snippetid = snippetid1,
         text      = text1) %>%
  distinct()

# extract snippet 2 from each comparison
snippet_data2 <- allsentences %>%
  select(snippetid2,text2) %>%
  rename(snippetid = snippetid2,
         text      = text2) %>%
  distinct()

# stack both sets of snippets and de-duplicate
snippet_data <- bind_rows(snippet_data1,snippet_data2) %>%
  distinct() %>%
  mutate(snippetid = as.character(snippetid))

# examine raw text snippet pairs alongside their corresponding covariates to ensure 
# we fully understand how covariate generation occurs
# --------------------------------------------------------------------------------
# extract 'easier' data
covars_data <- job999866covars_chameleons$easier %>%
  rename(easier_id = ID) %>%
  # merge with 'harder' data
  bind_cols(.,job999866covars_chameleons$harder) %>%
  rename(harder_id = ID) %>%
  # confirm number of rows has remained the same
  verify(nrow(.) == 19430) %>%
  # join m:1 with covariates for 'easier' snippet
  left_join(pred_data, by = "easier_id", relationship = "many-to-one") %>%
  # prefix all covariates with 'easier_' to specify these are covariates for 'easier' snippet
  rename_with(~ paste0("easier_", .), -c(harder_id.x,harder_id.y,easier_id)) %>%
  select(-harder_id.y) %>%
  rename(harder_id = harder_id.x) %>%
  # join m:1 with covariates for 'harder' snippet
  left_join(pred_data, by = "harder_id", relationship = "many-to-one") %>%
  # prefix all covariates with 'harder_' to specify these are covariates corresponding to 'harder' snippet
  rename_with(~ paste0("harder_", .), -c(starts_with("harder"),starts_with("easier"))) %>%
  select(-easier_id.y) %>%
  rename(easier_id = easier_id.x) %>%
  # merge on full text for easier snippets
  left_join(snippet_data, by = c("easier_id" = "snippetid")) %>%
  rename(easier_text = text) %>%
  # merge on full text for harder snippets
  left_join(snippet_data, by = c("harder_id" = "snippetid")) %>%
  rename(harder_text = text) %>%
  # confirm all snippets matched with their corresponding text
  verify(!is.na(easier_text) & !is.na(harder_text)) %>%
  # move text extracts up to front of file
  relocate(easier_text, .after = easier_id) %>%
  relocate(harder_text, .after = harder_id) 

# examine first row of data as an example
covars_data %>%
  head(1) %>%
  datatable(options = list(scrollX='400px'))

```

### Step 3. Run Bradley-Terry unstructured models

```{r}
#| warning: false
#| output: asis
#| code-fold: false

# covariate data from above
dat <- job999866covars_chameleons

# load respective author workspaces
load(file.path(author_rep,"BT_unstructured_brT_abilities.rda"))
load(file.path(author_rep,"BT_unstructured_brF_abilities.rda"))

# fit unstructured model with bias reduction (br=T) [not re-running because of runtime; loading directly from replication package]
#BT1 <-
#     BTm(player1 = easier, 
#         player2 = harder, 
#         br = TRUE, 
#         id = "ID", 
#         data = dat)

BT1 <- BT_unstruc_brT

# fit unstructured model without bias reduction (br=F) [not re-running because of run time; loading directly from replication package]
#BT2 <- 
#     BTm(player1 = easier, 
#         player2 = harder, 
#         br = FALSE, 
#         id = "ID", 
#         data = dat)

BT2 <- BT_unstruc_brF

# examine first 10 rows of baseline "ability" (easiness) as estimated from Bradley-Terry model
BTabilities(BT1) %>%
  head(10) %>%
  datatable()

```

### Step 4. Run Random Forest model for covariate feature selection

BT Model with reduced random forest features (with bias reduction).

```{r}
#| warning: false
#| output: asis
#| code-fold: false

# Y target feature: text easiness ("ability")
# extract ability column from bias-reduced BT model
y  <- BTabilities(BT1)[, "ability"] 

# remove NAs
yy <- y[!is.na(y)] 

# return row number, match left to right 
m <- match(names(yy), rownames(dat$predictors)) 

# collect the possible terms -- "note that we remove Flesch (because it's aliased by the other variables)"
terms <- c("W3Sy", "W2Sy", "W_1Sy", "W6C", "W7C", "W_wl.Dale.Chall", "Wlt3Sy", 
           "meanSentenceLength", "meanWordSyllables", "meanWordChars", 
           "meanSentenceChars", "meanSentenceSyllables", "brown_mean", "brown_min", 
           "google_mean_2000", "google_min_2000", "pr_noun", "pr_verb", "pr_adjective", 
           "pr_adverb", "pr_clause", "pr_sentence")

# X features for random forest
X <- dat$predictors[m, terms]

# run random forest for bias-reduced model
mod_bias_reduced <- randomForest(X, y = yy, ntree = 1000)

# display feature importance in descending order on node purity
import_bias_reduced = round(importance(mod_bias_reduced), 2)

as.data.frame(import_bias_reduced) %>%
  rownames_to_column() %>%
  arrange(-IncNodePurity) %>%
  rename(Feature       = rowname,
         `Node Purity` = IncNodePurity) %>%
  kbl(full_width = T, 
      align = "l",
      booktabs = T) %>%
  kable_styling(latex_options = c("repeat_header","scale_down","HOLD_position"), font_size = 12) 

```

### Step 5. Fit 4 main structured Bradley-Terry models

```{r}
#| warning: false
#| output: asis
#| code-fold: false

# load respective author workspace
#load(file.path(author_rep,"job999866covars_chameleons.rda"))

# Bradley-Terry 1: control only for baseline FRE score
BT_basic_Flesch <- BTm(player1 = easier, player2 = harder,
                       formula = ~ Flesch[ID], id = "ID",
                       data = job999866covars_chameleons)

# Bradley-Terry 2: control for FRE components (sentence length, word syllables), allowing for reweighting
BT_optimal_Flesch <- BTm(player1 = easier, player2 = harder, 
                         formula = ~ meanSentenceLength[ID] + meanWordSyllables[ID], 
                         id = "ID", data = job999866covars_chameleons)

# Bradley-Terry 3: control for 3 of 4 top covariates identified from random forest feature selection
BT_basic_RF <- BTm(player1 = easier, player2 = harder, 
                   formula = ~ google_min_2000[ID] + meanSentenceChars[ID] + pr_noun[ID], 
                   id ="ID", data = job999866covars_chameleons)

# Bradley-Terry 4: control for 4 top covariates identified from random forest feature selection
BT_best <- BTm(player1 = easier, player2 = harder, 
               formula = ~ google_min_2000[ID] +  meanSentenceChars[ID] + pr_noun[ID] + meanWordChars[ID], 
               id = "ID", data = job999866covars_chameleons)

# save results from four models
model_results <- list(BT_basic_Flesch = BT_basic_Flesch, 
                      BT_basic_RF = BT_basic_RF, 
                      BT_optimal_Flesch = BT_optimal_Flesch, 
                      BT_best = BT_best)

```

### Step 6. Create model results table

Corresponds to **Table 2** in paper.

```{r}
#| warning: false
#| output: asis
#| code-fold: false

# put models in a list
models <- list("FRE Baseline"   = BT_basic_Flesch,
               "FRE Reweight"   = BT_optimal_Flesch,
               "Basic RF Model" = BT_basic_RF,
               "Best Model"     = BT_best)

# specify table formatting
fmt <- list(
       list("raw" = "Num.Obs.", "clean" = "N", "fmt" = 0))

# produce LaTeX table
msummary(models,
         escape = FALSE,
         title = "Table 2. Comparing the Performance of the Structured Models",
         coef_rename = c("Flesch[ID]"               = "FRE",
                        "meanSentenceLength[ID]"    = "meanSentenceLength",
                        "meanWordSyllables[ID]"     = "meanWordSyllables",
                        "google_min_2000[ID]"       = "google_min",
                        "meanSentenceChars[ID]"     = "meanSentenceChars",
                        "pr_noun[ID]"               = "pr_noun",
                        "meanWordChars[ID]"         = "meanWordChars"))

```

### Step 7. Generate figure comparing authors' model to FRE

Corresponds to **Figure 1** in paper.

```{r}
#| warning: false
#| output: asis
#| code-fold: false

# get FRE scores for the snippets
dat <- job999866covars_chameleons
FRE <- dat$predictors$Flesch

names(FRE) <- rownames(dat$predictors)

# get lambdas from best fitting model
main_lambdas <- BTabilities(BT_best)[,"ability"]

# rescale lambdas to the 0-100 space correctly
rescaled_lambdas <- 226.06927 + 57.93899 * main_lambdas

# check that they are matched up
m <- match(names(FRE), names(rescaled_lambdas)) ## they are matched up

# produce plot
ggplot(data.frame(FRE = FRE, rslambda = rescaled_lambdas), aes(x = FRE, y = rslambda)) +
    geom_point(size = .6) +
    labs(y = "Rescaled Best BT Model") +
    geom_smooth(method = "lm", se = TRUE) +
    geom_hline(yintercept = c(0, 100), linetype = "dashed", color = "firebrick") +
    theme(axis.text.x = element_text(size = 5),
          axis.text.y = element_text(size = 5)) +
    theme_classic()

```
