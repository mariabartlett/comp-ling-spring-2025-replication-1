---
title: A Replication of _Measuring and Explaining Political Sophistication through Textual Complexity_ (Benoit et al., 2019)
subtitle: "PPOL 6801: Text as Data: Computational Linguistics"
author: "Wendy Shi & Maria Bartlett"
date: "February 11, 2025"
format:
  revealjs: 
    theme: bootstrap.scss
editor: visual
---

```{r}

# clear global environment
rm(list = ls())

# set seed
set.seed(12345)

# load packages
library(tidyverse)

# set relative paths
#root <- "C:/Users/mbart/OneDrive/Documents/Georgetown/04-Spring-2025/03-Computational-Linguistics/03_ProblemSets/01_PS1"

# define kable styling options
kable_style <- function(df) {
  
  df %>%
  kbl(full_width = T, 
      align = "l",
      booktabs = T) %>%
  kable_styling(latex_options = c("repeat_header","scale_down","HOLD_position")) 
}

```

# Introduction 

## Background {.smaller}

Authors seek to contribute to a key gap in literature: **How to empirically measure the sophistication of [_political_]{style="color: #FFC107"} texts?**

- _Why is sophistication important to measure?_

  - Concern that political communication is becoming too simplistic

  - Interest in studying potential causality between textual clarity and social science outcomes (e.g., levels of voter awareness)

- _What measures already exist?_

  - Many measures emerged from [_educational and psychology research_]{style="color: #FFC107"}, most popularly the [**Flesch Reading Ease (FRE)** score]{style="color: #FFC107"} (0-121 scale)
  
  - Nevertheless, these measures have been used in a range of research, including in political text analysis

$$
\small
FRE = 206.835 - 1.015(\frac{total\ number\ of\ words}{total\ number\ of\ sentences}) \small - 84.6(\frac{total\ number\ of\ syllables}{total\ number\ of\ words})
$$


## Background {.smaller}

- _What are the issues with using FRE and similar measures in political science research?_

  - **Not domain-specific:** Developed in 1948 to assess U.S. student reading proficiency
  
  - **Strong assumptions of what composes textual sophistication:** For example, FRE formula does not directly include a measure for frequently- vs. rarely-used words

  - **No uncertainty mechanism:** Not feasible to construct standard errors for estimates
  
  - **No natural ability to make comparisons:** FRE score units do not carry inherent meaning and there is no mechanism to convert scores into probabilities 

## Roadmap of Authors' Goals {.smaller}

**These questions lead the authors to two main goals:**

1. Re-explore determinants of textual sophistication, specifically in a political context

2. Develop a model for political textual complexity that can be applied to any political text

**Key connections to our coursework:**

1. Re-thinking measures of textual ease

2. Employment of supervised learning models in textual analysis

## Roadmap of Authors' Goals {.smaller}

**Components of goal 1 (Re-explore determinants of textual sophistication, specifically in a political context)**:

- Construct a corpus of political texts (level = snippets)

- Employ **human coders** to analyze snippets and make determinations of textual easiness [(outcome variable)]{style="color: #FFC107"}

- Calculate a range of statistics for corpus snippets (e.g., How long is the text? How many nouns? What is the easiness using FRE?) [(covariates)]{style="color: #FFC107"}

- Fit a machine learning model using outcome and covariates to identify which covariates are most predictive of text ease

## Roadmap of Authors' Goals {.smaller}

**Components of goal 2 (Develop a model for political textual complexity that can be applied to any political text)**:

- Run Bradley-Terry model using multiple model specifications (e.g., using covariates as specified from machine learning feature selection)

- Choose best-performing model

- Run this model on any political science text to assess (with mechanisms for uncertainty):

  - Between two texts, which is easier

  - Easiness as compared to a benchmark

# Methods

## Data {.smaller}

- Corpus: 70 State of the Union (SOTU) speeches post-1950

- Divided into "snippets" 

- Given two snippets, manual coders determine "easier" snippet (no ties!):

  -   Pairwise comparisons of snippets of similar length
  
  -   At least 3 coders per pair

  -   No snippet appears in isolation, thus make comparison between all pairs possible.

  -   7,236 total pairings for comparison (836 gold questions and 310 screeners to test coder fidelity/attention)

-   **Grand total of 19,810 comparisons**
  
## Sample of a text comparison
  
**Text 1:**

```         
"Under my Executive Order 12044, we required agencies to analyze the costs of their major new rules and consider alternative approaches-such as performance standards and voluntary codes-that may make rules less costly and more flexible.  We created the Regulatory Analysis Review Group in the White House to analyze the most costly proposed new rules and find ways to improve them." -- Carter 1981
```

**Text 2:**

```         
"We also show compassion abroad because regions overwhelmed by poverty, corruption, and despair are sources of terrorism and organized crime and human trafficking and the drug trade.  In recent years, you and I have taken unprecedented action to fight AIDS and malaria, expand the education of girls, and reward developing nations that are moving forward with economic and political reform." -- Bush 2006
```

## Bradley-Terry Model {.smaller}

- Model is designed for pairwise comparison (traditionally used to measure "ability")

- Authors leverage Bradley-Terry design to compute easiness $(a_i)$ of a text

- If the easiness of snippet $i$ s easier than the snippet $j$, the possibility of $i$ easier than $j$ is $a_i/a_j$

- Define $\lambda_i = log\ a_i$

$$
Logit\ [Pr(i\ easier\ than\ j)] = \lambda_i - \lambda_j \\
= log (\frac{a_i}{a_j})
$$

## Model covariates {.smaller}

Authors assume that a complex text uses:

-   Lengthier words more often

-   Rarer words more often **(keeping in mind that word rarity differs depending on time period)**

-   Lengthier sentences

-   Advanced grammar and syntax organization (e.g., subordinate clauses)

## Complete list of potential covariates {.smaller}

```{r, echo = FALSE}
library(png)
library(grid)

img <- readPNG("table1.png")
grid.raster(img)

```

## Covariate generation {.smaller}

```{r, echo = TRUE, eval = FALSE}

library(quanteda)
library(sophistication)
library(BradleyTerry2)
library(tidyverse)

# load human-coded data of SOTU passages from CrowdFlower
allsentences <- rbind(read.csv(file.path(author_rep,"CF_output_f999866.csv"), stringsAsFactors = FALSE),
                      read.csv(file.path(author_rep,"CF_output_f952737.csv"), stringsAsFactors = FALSE)) 

# add covariates to prepare data for BT Model
job999866covars_chameleons <-
    bt_input_make(allsentences, 
                  covars = TRUE,
                  # measures text difficulty
                  readability_measure = c("Flesch", 
                                          "Dale.Chall",
                                          "FOG", 
                                          "SMOG",
                                          "Spache",
                                          "Coleman.Liau"),
                  # measures of word rarity (Google and Brown)
                  covars_baseline = TRUE, 
                  # measures of parts of speech
                  covars_pos = TRUE,
                  normalize = TRUE)

```

## Snapshot: Data at this point in time {.smaller}

```{r}
#| warning: false
#| output: asis
#| echo: false
#| eval: true

# set seed
set.seed(12345)

# provide instruction for how to install user-written packages
#devtools::install_github("quanteda/spacyr", build_vignettes = FALSE)
#library(spacyr)
#spacy_install()
#spacy_initialize()
#devtools::install_github("kbenoit/sophistication")

# load packages
library(tidyverse)
library(spacyr)
library(sophistication)  
library(BradleyTerry2)
library(modelsummary)
library(quanteda)
library(quanteda.textstats)
library(assertr)
library(DT)
library(randomForest) 
library(kableExtra)

# set root path to working directory
root       <- "C:/Users/mbart/OneDrive/Documents/Georgetown/04-Spring-2025/03-Computational-Linguistics/04_Replications"
author_rep <- file.path(root,"02_DataverseMaterials","dataverse_files")

# load human-coded data of SOTU passages from CrowdFlower
allsentences <- rbind(read.csv(file.path(author_rep,"CF_output_f999866.csv"), stringsAsFactors = FALSE),
                      read.csv(file.path(author_rep,"CF_output_f952737.csv"), stringsAsFactors = FALSE)) %>%
  # confirm file is unique on id level
  verify(anyDuplicated(select(.,c(X_id))) == 0) %>%
  # check dimensions of data
  verify(nrow(.) == 27807 & ncol(.) == 25) %>%
  # confirm all tainted responses have been removed
  verify(X_tainted == "false")

# CrowdFlower data contains 3,322 text snippets derived from 226 SOTU speeches
stopifnot(length(unique(c(allsentences$snippetid1,allsentences$snippetid2))) == 3322)
stopifnot(length(unique(c(allsentences$docid1,allsentences$docid2))) == 226)

# add covariates to prepare data for BT Model
job999866covars_chameleons <-
    bt_input_make(allsentences, 
                  # add covariates for each snippet, taken directly from Crowdflower saved data
                  covars = TRUE,
                  # specify measures text difficulty
                  readability_measure = c("Flesch", 
                                          "Dale.Chall",
                                          "FOG", 
                                          "SMOG",
                                          "Spache",
                                          "Coleman.Liau"),
                  # add summary baseline frequencies compared to Google and Brown corpora speech
                  # computed by covars_make_pos()
                  covars_baseline = TRUE, 
                  # add frequencies of parts of speech computed by covars_make_pos()
                  covars_pos = TRUE, 
                  # return appropriately normalized covariates (including parts of speech if applicable)
                  normalize = TRUE)

# merge data to be sure we understand how data is working
pred_data <- job999866covars_chameleons$predictors %>%
  mutate(easier_id = row.names(.),
         harder_id = row.names(.))

snippet_data1 <- allsentences %>%
  select(snippetid1,text1) %>%
  rename(snippetid = snippetid1,
         text      = text1) %>%
  distinct()

snippet_data2 <- allsentences %>%
  select(snippetid2,text2) %>%
  rename(snippetid = snippetid2,
         text      = text2) %>%
  distinct()

snippet_data <- bind_rows(snippet_data1,snippet_data2) %>%
  distinct() %>%
  mutate(snippetid = as.character(snippetid))

covars_data <- job999866covars_chameleons$easier %>%
  rename(easier_id = ID) %>%
  bind_cols(.,job999866covars_chameleons$harder) %>%
  rename(harder_id = ID) %>%
  verify(nrow(.) == 19430) %>%
  left_join(pred_data, by = "easier_id", relationship = "many-to-one") %>%
  rename_with(~ paste0("easier_", .), -c(harder_id.x,harder_id.y,easier_id)) %>%
  select(-harder_id.y) %>%
  rename(harder_id = harder_id.x) %>%
  left_join(pred_data, by = "harder_id", relationship = "many-to-one") %>%
  rename_with(~ paste0("harder_", .), -c(starts_with("harder"),starts_with("easier"))) %>%
  select(-easier_id.y) %>%
  rename(easier_id = easier_id.x) %>%
  left_join(snippet_data, by = c("easier_id" = "snippetid")) %>%
  rename(easier_text = text) %>%
  left_join(snippet_data, by = c("harder_id" = "snippetid")) %>%
  rename(harder_text = text) %>%
  verify(!is.na(easier_text) & !is.na(harder_text)) %>%
  relocate(easier_text, .after = easier_id) %>%
  relocate(harder_text, .after = harder_id) 

covars_data %>%
  head(1) %>%
  kbl(full_width = T, 
      align = "l",
      booktabs = T) %>%
  kable_styling(latex_options = c("repeat_header","scale_down","HOLD_position"), font_size = 12) 

```

## Random Forest for Feature Selection {.smaller}

- Authors run random forest model (1,000 trees) to identify covariates most predictive of text ease

- Based on node purity scores from random forest model, authors choose one covariate from each of their four categories of textual complexity:

  - **Long Words:** `meanWordSyllables`
  
  - **Rare Words:** `google_min`
  
  - **Long Sentences:** `meanSentenceChars`
  
  - **Complex Content:** `pr_noun`

## Run 4 model specifications {.smaller}

```{r, echo = TRUE, eval = FALSE}

# Bradley-Terry 1: control only for baseline FRE score
BT_basic_Flesch <- BTm(player1 = easier, player2 = harder,
                       formula = ~ Flesch[ID], id = "ID",
                       data = job999866covars_chameleons)

# Bradley-Terry 2: control for FRE components (sentence length, word syllables), allowing for reweighting
BT_optimal_Flesch <- BTm(player1 = easier, player2 = harder, 
                         formula = ~ meanSentenceLength[ID] + meanWordSyllables[ID], 
                         id = "ID", data = job999866covars_chameleons)

# Bradley-Terry 3: control for 3 of 4 top covariates identified from random forest feature selection
BT_basic_RF <- BTm(player1 = easier, player2 = harder, 
                   formula = ~ google_min_2000[ID] + meanSentenceChars[ID] + pr_noun[ID], 
                   id ="ID", data = job999866covars_chameleons)

# Bradley-Terry 4: control for 4 top covariates identified from random forest feature selection
BT_best <- BTm(player1 = easier, player2 = harder, 
               formula = ~ google_min_2000[ID] +  meanSentenceChars[ID] + pr_noun[ID] + meanWordChars[ID], 
               id = "ID", data = job999866covars_chameleons)
```

# Results

## Best model {.smaller}

**Model 4** (controlling for all four covariates from feature selection) performs best in terms of:

- Lowest Akaike information criterion (AIC)

- Highest proportion of comparisons accurately predicted

- $\hat\beta$s from model 4 are used to construct $\hat\lambda$ probabilities that a text is easier relative to another

## Best model {.smaller}

```{r}
#| warning: false
#| output: asis
#| echo: false
#| eval: false

# load packages
library(tidyverse)
library(spacyr)
library(sophistication)  
library(BradleyTerry2)
library(modelsummary)
library(quanteda)
library(quanteda.textstats)
library(assertr)
library(DT)
library(randomForest) 

# set root path to working directory
root       <- "C:/Users/mbart/OneDrive/Documents/Georgetown/04-Spring-2025/03-Computational-Linguistics/04_Replications"
author_rep <- file.path(root,"02_DataverseMaterials","dataverse_files")

# load workspace authors used
load(file.path(author_rep,"job999866covars_chameleons.rda"))

# run baseline Flesch model
BT_basic_Flesch <- BTm(player1 = easier, player2 = harder,
                       formula = ~ Flesch[ID], id = "ID",
                       data = job999866covars_chameleons)

# run optimal Flesch model
BT_optimal_Flesch <- BTm(player1 = easier, player2 = harder, 
                         formula = ~ meanSentenceLength[ID] + meanWordSyllables[ID], 
                         id = "ID", data = job999866covars_chameleons)

# run basic random forest model
BT_basic_RF <- BTm(player1 = easier, player2 = harder, 
                   formula = ~ google_min_2000[ID] + meanSentenceChars[ID] + pr_noun[ID], 
                   id ="ID", data = job999866covars_chameleons)

# run best model
BT_best <- BTm(player1 = easier, player2 = harder, 
               formula = ~ google_min_2000[ID] +  meanSentenceChars[ID] + pr_noun[ID] + meanWordChars[ID], 
               id = "ID", data = job999866covars_chameleons)

# save results from four models
model_results <- list(BT_basic_Flesch = BT_basic_Flesch, 
                      BT_basic_RF = BT_basic_RF, 
                      BT_optimal_Flesch = BT_optimal_Flesch, 
                      BT_best = BT_best)

# put models in a list
models <- list("FRE Baseline"   = BT_basic_Flesch,
               "FRE Reweight"   = BT_optimal_Flesch,
               "Basic RF Model" = BT_basic_RF,
               "Best Model"     = BT_best)

fmt <- list(
       list("raw" = "nobs", "clean" = "N", "fmt" = 0),
       list("raw" = "adj.r.squared", "clean" = "Adjusted $R^2$", "fmt" = 2),
       list("raw" = "rmse", "clean" = "$\\sigma$", "fmt" = 2))

# produce LaTeX table
msummary(models,
         escape = FALSE,
         title = "Table 2. Comparing the Performance of the Structured Models",
         coef_rename = c("Flesch[ID]"               = "FRE",
                        "meanSentenceLength[ID]"    = "meanSentenceLength",
                        "meanWordSyllables[ID]"     = "meanWordSyllables",
                        "google_min_2000[ID]"       = "google_min",
                        "meanSentenceChars[ID]"     = "meanSentenceChars",
                        "pr_noun[ID]"               = "pr_noun",
                        "meanWordChars[ID]"         = "meanWordChars"))
```

## Validation: Comparing to FRE {.smaller}

```{r, echo = FALSE}
library(png)
library(grid)

img <- readPNG("fig1.png")
grid.raster(img)

```

# Autopsy

## Autopsy {.smaller}

- Able to replicate feature selection results, model results, and key plots

- Authors wrote `sophistication` package and replication materials rely heavily on this package; had some initial start-up issues because this package is not actively maintained on GitHub

- One package authors use is no longer maintained on CRAN

# Extension

## Extension {.smaller}

**Code updates:**

- Modernized some of the approaches, particularly with data wrangling, to use more of the `tidyverse`

- Added data checks and amplified comments to add additional clarity

**Substantive proposals:**

- Improved model testing (model was trained on all coder data)

- Improved model validation beyond comparing against FRE score

- Run model on different political text (e.g., campaign speeches)

- Could consider other machine learning approaches besides random forest for feature selection

- With more resources (would require hiring coders!) change outcome variable in Bradley-Terry model to different measure of interest (e.g., tone)

## References {.smaller}

Benoit, Kenneth, Kevin Munger, and Arthur Spirling. 2019. "Measuring and Explaining Political Sophistication through Textual Complexity." _American Journal of Political Science_ 63 (2): 491–508.doi: 10.1111/ajps.12423

Benoit, Kenneth, 2019, "Replication Data for: Measuring and Explaining Political Sophistication Through Textual Complexity", https://doi.org/10.7910/DVN/9SF3TI, Harvard Dataverse, V1, UNF:6:3lWCX52gHXjVfaeDpmEBPQ== [fileUNF]
